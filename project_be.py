# -*- coding: utf-8 -*-
"""Project_BE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JOGGg9qHlugMnQtnlcX7l2yTAzhqZ3i4
"""

!pip install diffusers transformers accelerate torch

import torch
from diffusers import StableDiffusionXLPipeline, AutoPipelineForText2Image

# We are loading the "base" model first.
# This model is a 'float16' version, which runs efficiently on the free T4 GPU.
pipeline = AutoPipelineForText2Image.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    torch_dtype=torch.float16,
    variant="fp16",
    use_safetensors=True
).to("cuda")

print("Model has been loaded and moved to the GPU (cuda).")

from IPython.display import display

# 1. Define your prompt
prompt = "A knight in shining armor, cinematic lighting, fantasy art, high detail"

# 2. Run the pipeline!
# This is the line that actually tells the AI to generate the image.
image = pipeline(prompt=prompt).images[0]

# 3. Display the image
print("Image generation complete!")
display(image)

# (Optional) Save the image to your Colab environment
image.save("knight.png")

!pip install diffusers transformers accelerate torch torchvision
!pip install imageio[ffmpeg]

import torch
from diffusers import StableVideoDiffusionPipeline
from diffusers.utils import load_image, export_to_video

# 1. Load the SVD pipeline
# Note: This is a different model name from before!
video_pipeline = StableVideoDiffusionPipeline.from_pretrained(
    "stabilityai/stable-video-diffusion-img2vid-xt",
    torch_dtype=torch.float16,
    variant="fp16"
)

# 2. Move the video model to the GPU
video_pipeline.to("cuda")

# 3. (Optional but recommended) Enable special memory-saving features
# This helps the free T4 GPU run this big model without crashing.
video_pipeline.enable_model_cpu_offload()

print("Stable Video Diffusion model is loaded and ready.")

import torch
import gc

# 1. Delete the big 'pipeline' (SDXL) object from memory
# This will free up all the memory it was using
del pipeline

# 2. Run the Python garbage collector to clean up
gc.collect()

# 3. Ask PyTorch to fully empty its 'cache'
# This is the most important command
torch.cuda.empty_cache()

print("GPU memory has been cleared. You can now run the video generation.")

from diffusers.utils import load_image, export_to_video

# 1. Load the "knight.png" image you created in Phase 1
# This image must exist. If you changed its name, change it here too.
base_image = load_image("knight.png")

# 2. Set the video generation parameters
# These can be changed later, but let's use good defaults.
base_image.resize((1024, 576)) # Resize image to the format SVD expects
motion_bucket_id = 127         # Controls the amount of motion (higher is more)
noise_aug_strength = 0.02      # Controls how much the image is 'shaken'

# 3. Run the video pipeline!
# This is the line that actually generates the video frames.
frames = video_pipeline(
    base_image,
    decode_chunk_size=8,
    motion_bucket_id=motion_bucket_id,
    noise_aug_strength=noise_aug_strength,
).frames[0]

# 4. Save the frames as an .mp4 video file
export_to_video(frames, "knight_video.mp4", fps=7)

print("Video generation complete! 'knight_video.mp4' is saved.")

!pip install diffusers transformers accelerate torch torchvision
!pip install imageio[ffmpeg]

import torch
from diffusers import StableVideoDiffusionPipeline
from diffusers.utils import load_image, export_to_video

# 1. Load the SVD pipeline
video_pipeline = StableVideoDiffusionPipeline.from_pretrained(
    "stabilityai/stable-video-diffusion-img2vid-xt",
    torch_dtype=torch.float16,
    variant="fp16"
)

# 2. Move the video model to the GPU
video_pipeline.to("cuda")

# 3. Enable memory-saving features
video_pipeline.enable_model_cpu_offload()

print("Stable VideoDiffusion model is loaded and ready.")

from diffusers.utils import load_image, export_to_video

# 1. Load the "knight.png" image
# This assumes it's in the main folder.
base_image = load_image("knight.png")

# 2. Set the video generation parameters
base_image = base_image.resize((1024, 576)) # Resize image to the format SVD expects
motion_bucket_id = 127         # Controls the amount of motion
noise_aug_strength = 0.02      # Controls how much the image 'shakes'

# 3. Run the video pipeline!
# This should work now that we have plenty of memory.
frames = video_pipeline(
    base_image,
    decode_chunk_size=8,
    motion_bucket_id=motion_bucket_id,
    noise_aug_strength=noise_aug_strength,
).frames[0]

# 4. Save the frames as an .mp4 video file
export_to_video(frames, "knight_video.mp4", fps=7)

print("Video generation complete! 'knight_video.mp4' is saved.")

